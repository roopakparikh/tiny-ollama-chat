# ğŸ¤– Tiny Ollama Chat

A super small lightweight UI for accessing Ollama models. Just a working version now, will make some improvements in future.

## âœ¨ Features

- ğŸ“± Real-time message streaming
- ğŸ§  View AI thinking process
- ğŸ’¬ Conversation history
- ğŸš€ Multiple model support

## ğŸš¦ Getting Started

### Prerequisites
- Node.js
- Go
- Ollama running locally

### ğŸƒâ€â™‚ï¸ Installing

1. Clone the repository
```bash
git clone https://github.com/yourusername/tiny-ollama-chat.git
cd tiny-ollama-chat
```

2. Set up the frontend
```bash
cd client
npm install
npm run build
```

3.Set up the backend
```bash
cd ../server
go mod download
```

### ğŸƒâ€â™‚ï¸ Running the Application
```bash
go run cmd/server/main.go
```

The application will be available at http://localhost:8080

## ğŸ“– Usage

1. Select a model to start a new conversation
2. Type your message and send
3. Browse previous conversations in the sidebar

## ğŸ”® Coming Soon
- ğŸ”— Custom Ollama URL support
- ğŸ’¾ Support for multiple databases

## ğŸ’¡ Why Tiny Ollama?

Sometimes simpler is better! This minimal interface focuses on what matters most - having great conversations with AI without the bloat.
